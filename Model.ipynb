{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(list(newsgroups_train.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\",\n",
       " \"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(newsgroups_train.data[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    return [w for w in gensim.utils.simple_preprocess(text) if w not in gensim.parsing.preprocessing.STOPWORDS and len(w)>3]\n",
    "def lemmatize(text):\n",
    "    return [WordNetLemmatizer().lemmatize(w) for w in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_doc = []\n",
    "for s in newsgroups_train.data[:4]:\n",
    "    preproc_doc.append(lemmatize(preprocess(s)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preproc_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwords = gensim.corpora.Dictionary(preproc_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 addition\n",
      "1 body\n",
      "2 bricklin\n",
      "3 brought\n",
      "4 bumper\n",
      "5 called\n",
      "6 college\n",
      "7 door\n",
      "8 early\n",
      "9 engine\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for k,v in dwords.iteritems():\n",
    "    c+=1\n",
    "    print(k,v)\n",
    "    if c==10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwords.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'host'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwords.id2token[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = [dwords.doc2bow(s) for s in preproc_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 2),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 2),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1)],\n",
       " [(13, 1),\n",
       "  (18, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (28, 1),\n",
       "  (35, 1),\n",
       "  (37, 1),\n",
       "  (39, 1),\n",
       "  (42, 1),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1),\n",
       "  (48, 1),\n",
       "  (49, 1),\n",
       "  (50, 1),\n",
       "  (51, 2),\n",
       "  (52, 5),\n",
       "  (53, 1),\n",
       "  (54, 1),\n",
       "  (55, 1),\n",
       "  (56, 1),\n",
       "  (57, 2),\n",
       "  (58, 1),\n",
       "  (59, 2),\n",
       "  (60, 2),\n",
       "  (61, 1),\n",
       "  (62, 2),\n",
       "  (63, 1),\n",
       "  (64, 1),\n",
       "  (65, 1),\n",
       "  (66, 1),\n",
       "  (67, 1),\n",
       "  (68, 1),\n",
       "  (69, 1),\n",
       "  (70, 1),\n",
       "  (71, 1),\n",
       "  (72, 1),\n",
       "  (73, 3),\n",
       "  (74, 1),\n",
       "  (75, 1),\n",
       "  (76, 1),\n",
       "  (77, 1),\n",
       "  (78, 1),\n",
       "  (79, 1),\n",
       "  (80, 1),\n",
       "  (81, 1),\n",
       "  (82, 1),\n",
       "  (83, 1),\n",
       "  (84, 2),\n",
       "  (85, 1),\n",
       "  (86, 1),\n",
       "  (87, 2),\n",
       "  (88, 1),\n",
       "  (89, 1),\n",
       "  (90, 4)],\n",
       " [(14, 2),\n",
       "  (15, 1),\n",
       "  (18, 2),\n",
       "  (20, 1),\n",
       "  (26, 1),\n",
       "  (35, 1),\n",
       "  (37, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (55, 2),\n",
       "  (59, 1),\n",
       "  (63, 1),\n",
       "  (70, 1),\n",
       "  (86, 1),\n",
       "  (91, 1),\n",
       "  (92, 1),\n",
       "  (93, 1),\n",
       "  (94, 1),\n",
       "  (95, 1),\n",
       "  (96, 3),\n",
       "  (97, 1),\n",
       "  (98, 1),\n",
       "  (99, 1),\n",
       "  (100, 1),\n",
       "  (101, 2),\n",
       "  (102, 1),\n",
       "  (103, 1),\n",
       "  (104, 1),\n",
       "  (105, 1),\n",
       "  (106, 1),\n",
       "  (107, 3),\n",
       "  (108, 1),\n",
       "  (109, 1),\n",
       "  (110, 1),\n",
       "  (111, 1),\n",
       "  (112, 1),\n",
       "  (113, 2),\n",
       "  (114, 1),\n",
       "  (115, 1),\n",
       "  (116, 1),\n",
       "  (117, 1),\n",
       "  (118, 1),\n",
       "  (119, 1),\n",
       "  (120, 1),\n",
       "  (121, 1),\n",
       "  (122, 1),\n",
       "  (123, 3),\n",
       "  (124, 1),\n",
       "  (125, 1),\n",
       "  (126, 1),\n",
       "  (127, 1),\n",
       "  (128, 1),\n",
       "  (129, 1),\n",
       "  (130, 1),\n",
       "  (131, 1),\n",
       "  (132, 1),\n",
       "  (133, 1),\n",
       "  (134, 3),\n",
       "  (135, 1),\n",
       "  (136, 1),\n",
       "  (137, 1),\n",
       "  (138, 1),\n",
       "  (139, 1),\n",
       "  (140, 1),\n",
       "  (141, 1),\n",
       "  (142, 2),\n",
       "  (143, 1),\n",
       "  (144, 1),\n",
       "  (145, 1),\n",
       "  (146, 1),\n",
       "  (147, 1),\n",
       "  (148, 1),\n",
       "  (149, 3),\n",
       "  (150, 1),\n",
       "  (151, 1),\n",
       "  (152, 1),\n",
       "  (153, 1),\n",
       "  (154, 4),\n",
       "  (155, 3),\n",
       "  (156, 1),\n",
       "  (157, 1),\n",
       "  (158, 1),\n",
       "  (159, 1),\n",
       "  (160, 1),\n",
       "  (161, 1),\n",
       "  (162, 1),\n",
       "  (163, 1),\n",
       "  (164, 2),\n",
       "  (165, 1),\n",
       "  (166, 1),\n",
       "  (167, 1),\n",
       "  (168, 2),\n",
       "  (169, 1),\n",
       "  (170, 1),\n",
       "  (171, 1),\n",
       "  (172, 1),\n",
       "  (173, 1),\n",
       "  (174, 1),\n",
       "  (175, 1),\n",
       "  (176, 1),\n",
       "  (177, 2),\n",
       "  (178, 1),\n",
       "  (179, 1),\n",
       "  (180, 1),\n",
       "  (181, 2),\n",
       "  (182, 1)],\n",
       " [(13, 1),\n",
       "  (15, 1),\n",
       "  (18, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (28, 1),\n",
       "  (35, 1),\n",
       "  (38, 1),\n",
       "  (45, 1),\n",
       "  (71, 1),\n",
       "  (108, 1),\n",
       "  (132, 1),\n",
       "  (133, 1),\n",
       "  (183, 2),\n",
       "  (184, 1),\n",
       "  (185, 2),\n",
       "  (186, 2),\n",
       "  (187, 1),\n",
       "  (188, 1),\n",
       "  (189, 1),\n",
       "  (190, 2),\n",
       "  (191, 1),\n",
       "  (192, 1),\n",
       "  (193, 2),\n",
       "  (194, 4),\n",
       "  (195, 1),\n",
       "  (196, 2),\n",
       "  (197, 1),\n",
       "  (198, 2),\n",
       "  (199, 1),\n",
       "  (200, 1),\n",
       "  (201, 1),\n",
       "  (202, 1),\n",
       "  (203, 1),\n",
       "  (204, 1),\n",
       "  (205, 1),\n",
       "  (206, 1),\n",
       "  (207, 1),\n",
       "  (208, 1),\n",
       "  (209, 1),\n",
       "  (210, 1),\n",
       "  (211, 1),\n",
       "  (212, 1),\n",
       "  (213, 1),\n",
       "  (214, 1),\n",
       "  (215, 2),\n",
       "  (216, 1),\n",
       "  (217, 1),\n",
       "  (218, 3),\n",
       "  (219, 1),\n",
       "  (220, 1),\n",
       "  (221, 1),\n",
       "  (222, 1)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.16607175561867923),\n",
      " (1, 0.16607175561867923),\n",
      " (2, 0.16607175561867923),\n",
      " (3, 0.16607175561867923),\n",
      " (4, 0.16607175561867923),\n",
      " (5, 0.16607175561867923),\n",
      " (6, 0.16607175561867923),\n",
      " (7, 0.33214351123735847),\n",
      " (8, 0.16607175561867923),\n",
      " (9, 0.16607175561867923),\n",
      " (10, 0.16607175561867923),\n",
      " (11, 0.16607175561867923),\n",
      " (12, 0.16607175561867923),\n",
      " (13, 0.034463003076411945),\n",
      " (14, 0.08303587780933962),\n",
      " (15, 0.034463003076411945),\n",
      " (16, 0.16607175561867923),\n",
      " (17, 0.33214351123735847),\n",
      " (19, 0.16607175561867923),\n",
      " (20, 0.08303587780933962),\n",
      " (21, 0.16607175561867923),\n",
      " (22, 0.16607175561867923),\n",
      " (23, 0.16607175561867923),\n",
      " (24, 0.16607175561867923),\n",
      " (25, 0.034463003076411945),\n",
      " (27, 0.16607175561867923),\n",
      " (28, 0.034463003076411945),\n",
      " (29, 0.16607175561867923),\n",
      " (30, 0.16607175561867923),\n",
      " (31, 0.16607175561867923),\n",
      " (32, 0.16607175561867923),\n",
      " (33, 0.16607175561867923),\n",
      " (34, 0.16607175561867923),\n",
      " (36, 0.16607175561867923),\n",
      " (37, 0.034463003076411945),\n",
      " (38, 0.08303587780933962),\n",
      " (39, 0.034463003076411945),\n",
      " (40, 0.08303587780933962),\n",
      " (41, 0.16607175561867923)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow)\n",
    "corpus_tfidf = tfidf[bow]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.004*\"plus\" + 0.004*\"played\" + 0.004*\"price\" + 0.004*\"premium\" + 0.004*\"powerbook\" + 0.004*\"post\" + 0.004*\"prove\" + 0.004*\"opinion\" + 0.004*\"perform\" + 0.004*\"purdue\"\n",
      "Topic: 1 Word: 0.004*\"plus\" + 0.004*\"played\" + 0.004*\"price\" + 0.004*\"premium\" + 0.004*\"powerbook\" + 0.004*\"post\" + 0.004*\"prove\" + 0.004*\"opinion\" + 0.004*\"perform\" + 0.004*\"purdue\"\n",
      "Topic: 2 Word: 0.031*\"clock\" + 0.025*\"washington\" + 0.020*\"poll\" + 0.014*\"speed\" + 0.014*\"experience\" + 0.014*\"carson\" + 0.014*\"guykuo\" + 0.014*\"floppy\" + 0.014*\"upgrade\" + 0.009*\"sink\"\n",
      "Topic: 3 Word: 0.004*\"plus\" + 0.004*\"played\" + 0.004*\"price\" + 0.004*\"premium\" + 0.004*\"powerbook\" + 0.004*\"post\" + 0.004*\"prove\" + 0.004*\"opinion\" + 0.004*\"perform\" + 0.004*\"purdue\"\n",
      "Topic: 4 Word: 0.004*\"plus\" + 0.004*\"played\" + 0.004*\"price\" + 0.004*\"premium\" + 0.004*\"powerbook\" + 0.004*\"post\" + 0.004*\"prove\" + 0.004*\"opinion\" + 0.004*\"perform\" + 0.004*\"purdue\"\n",
      "Topic: 5 Word: 0.004*\"plus\" + 0.004*\"played\" + 0.004*\"price\" + 0.004*\"premium\" + 0.004*\"powerbook\" + 0.004*\"post\" + 0.004*\"prove\" + 0.004*\"opinion\" + 0.004*\"perform\" + 0.004*\"purdue\"\n",
      "Topic: 6 Word: 0.004*\"plus\" + 0.004*\"played\" + 0.004*\"price\" + 0.004*\"premium\" + 0.004*\"powerbook\" + 0.004*\"post\" + 0.004*\"prove\" + 0.004*\"opinion\" + 0.004*\"perform\" + 0.004*\"purdue\"\n",
      "Topic: 7 Word: 0.028*\"harris\" + 0.022*\"weitek\" + 0.015*\"division\" + 0.015*\"jgreen\" + 0.015*\"green\" + 0.015*\"abraxis\" + 0.015*\"iastate\" + 0.015*\"system\" + 0.015*\"amber\" + 0.015*\"chip\"\n",
      "Topic: 8 Word: 0.004*\"plus\" + 0.004*\"played\" + 0.004*\"price\" + 0.004*\"premium\" + 0.004*\"powerbook\" + 0.004*\"post\" + 0.004*\"prove\" + 0.004*\"opinion\" + 0.004*\"perform\" + 0.004*\"purdue\"\n",
      "Topic: 9 Word: 0.004*\"plus\" + 0.004*\"played\" + 0.004*\"price\" + 0.004*\"premium\" + 0.004*\"powerbook\" + 0.004*\"post\" + 0.004*\"prove\" + 0.004*\"opinion\" + 0.004*\"perform\" + 0.004*\"purdue\"\n",
      "Topic: 10 Word: 0.004*\"plus\" + 0.004*\"played\" + 0.004*\"price\" + 0.004*\"premium\" + 0.004*\"powerbook\" + 0.004*\"post\" + 0.004*\"prove\" + 0.004*\"opinion\" + 0.004*\"perform\" + 0.004*\"purdue\"\n",
      "Topic: 11 Word: 0.004*\"plus\" + 0.004*\"played\" + 0.004*\"price\" + 0.004*\"premium\" + 0.004*\"powerbook\" + 0.004*\"post\" + 0.004*\"prove\" + 0.004*\"opinion\" + 0.004*\"perform\" + 0.004*\"purdue\"\n",
      "Topic: 12 Word: 0.004*\"plus\" + 0.004*\"played\" + 0.004*\"price\" + 0.004*\"premium\" + 0.004*\"powerbook\" + 0.004*\"post\" + 0.004*\"prove\" + 0.004*\"opinion\" + 0.004*\"perform\" + 0.004*\"purdue\"\n",
      "Topic: 13 Word: 0.004*\"plus\" + 0.004*\"played\" + 0.004*\"price\" + 0.004*\"premium\" + 0.004*\"powerbook\" + 0.004*\"post\" + 0.004*\"prove\" + 0.004*\"opinion\" + 0.004*\"perform\" + 0.004*\"purdue\"\n",
      "Topic: 14 Word: 0.004*\"plus\" + 0.004*\"played\" + 0.004*\"price\" + 0.004*\"premium\" + 0.004*\"powerbook\" + 0.004*\"post\" + 0.004*\"prove\" + 0.004*\"opinion\" + 0.004*\"perform\" + 0.004*\"purdue\"\n",
      "Topic: 15 Word: 0.004*\"plus\" + 0.004*\"played\" + 0.004*\"price\" + 0.004*\"premium\" + 0.004*\"powerbook\" + 0.004*\"post\" + 0.004*\"prove\" + 0.004*\"opinion\" + 0.004*\"perform\" + 0.004*\"purdue\"\n",
      "Topic: 16 Word: 0.015*\"door\" + 0.015*\"lerxst\" + 0.014*\"purdue\" + 0.011*\"anybody\" + 0.011*\"powerbook\" + 0.011*\"machine\" + 0.011*\"display\" + 0.011*\"heard\" + 0.011*\"question\" + 0.008*\"model\"\n",
      "Topic: 17 Word: 0.004*\"plus\" + 0.004*\"played\" + 0.004*\"price\" + 0.004*\"premium\" + 0.004*\"powerbook\" + 0.004*\"post\" + 0.004*\"prove\" + 0.004*\"opinion\" + 0.004*\"perform\" + 0.004*\"purdue\"\n",
      "Topic: 18 Word: 0.004*\"plus\" + 0.004*\"played\" + 0.004*\"price\" + 0.004*\"premium\" + 0.004*\"powerbook\" + 0.004*\"post\" + 0.004*\"prove\" + 0.004*\"opinion\" + 0.004*\"perform\" + 0.004*\"purdue\"\n",
      "Topic: 19 Word: 0.004*\"plus\" + 0.004*\"played\" + 0.004*\"price\" + 0.004*\"premium\" + 0.004*\"powerbook\" + 0.004*\"post\" + 0.004*\"prove\" + 0.004*\"opinion\" + 0.004*\"perform\" + 0.004*\"purdue\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=20, id2word=dwords, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9334943294525146\t Topic: 0.015*\"door\" + 0.015*\"lerxst\" + 0.014*\"purdue\" + 0.011*\"anybody\" + 0.011*\"powerbook\" + 0.011*\"machine\" + 0.011*\"display\" + 0.011*\"heard\" + 0.011*\"question\" + 0.008*\"model\"\n"
     ]
    }
   ],
   "source": [
    "bow_test = dwords.doc2bow(lemmatize(preprocess(newsgroups_test.data[0])))\n",
    "\n",
    "for index, score in sorted(lda_model_tfidf[bow_test], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_test.target[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.9758</td>\n",
       "      <td>door, lerxst, purdue, anybody, powerbook, mach...</td>\n",
       "      <td>[lerxst, thing, subject, nntp, posting, host, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9855</td>\n",
       "      <td>clock, washington, poll, speed, experience, ca...</td>\n",
       "      <td>[guykuo, carson, washington, subject, clock, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.9922</td>\n",
       "      <td>door, lerxst, purdue, anybody, powerbook, mach...</td>\n",
       "      <td>[twillis, purdue, thomas, willis, subject, que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>harris, weitek, division, jgreen, green, abrax...</td>\n",
       "      <td>[jgreen, amber, green, subject, weitek, organi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0            16.0              0.9758   \n",
       "1            1             2.0              0.9855   \n",
       "2            2            16.0              0.9922   \n",
       "3            3             7.0              0.9841   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  door, lerxst, purdue, anybody, powerbook, mach...   \n",
       "1  clock, washington, poll, speed, experience, ca...   \n",
       "2  door, lerxst, purdue, anybody, powerbook, mach...   \n",
       "3  harris, weitek, division, jgreen, green, abrax...   \n",
       "\n",
       "                                                Text  \n",
       "0  [lerxst, thing, subject, nntp, posting, host, ...  \n",
       "1  [guykuo, carson, washington, subject, clock, p...  \n",
       "2  [twillis, purdue, thomas, willis, subject, que...  \n",
       "3  [jgreen, amber, green, subject, weitek, organi...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model_tfidf, corpus=bow, texts=preproc_doc)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\n",
      "Subject: PB questions...\n",
      "Organization: Purdue University Engineering Computer Network\n",
      "Distribution: usa\n",
      "Lines: 36\n",
      "\n",
      "well folks, my mac plus finally gave up the ghost this weekend after\n",
      "starting life as a 512k way back in 1985.  sooo, i'm in the market for a\n",
      "new machine a bit sooner than i intended to be...\n",
      "\n",
      "i'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\n",
      "of questions that (hopefully) somebody can answer:\n",
      "\n",
      "* does anybody know any dirt on when the next round of powerbook\n",
      "introductions are expected?  i'd heard the 185c was supposed to make an\n",
      "appearence \"this summer\" but haven't heard anymore on it - and since i\n",
      "don't have access to macleak, i was wondering if anybody out there had\n",
      "more info...\n",
      "\n",
      "* has anybody heard rumors about price drops to the powerbook line like the\n",
      "ones the duo's just went through recently?\n",
      "\n",
      "* what's the impression of the display on the 180?  i could probably swing\n",
      "a 180 if i got the 80Mb disk rather than the 120, but i don't really have\n",
      "a feel for how much \"better\" the display is (yea, it looks great in the\n",
      "store, but is that all \"wow\" or is it really that good?).  could i solicit\n",
      "some opinions of people who use the 160 and 180 day-to-day on if its worth\n",
      "taking the disk size and money hit to get the active display?  (i realize\n",
      "this is a real subjective question, but i've only played around with the\n",
      "machines in a computer store breifly and figured the opinions of somebody\n",
      "who actually uses the machine daily might prove helpful).\n",
      "\n",
      "* how well does hellcats perform?  ;)\n",
      "\n",
      "thanks a bunch in advance for any info - if you could email, i'll post a\n",
      "summary (news reading time is at a premium with finals just around the\n",
      "corner... :( )\n",
      "--\n",
      "Tom Willis  \\  twillis@ecn.purdue.edu    \\    Purdue Electrical Engineering\n",
      "---------------------------------------------------------------------------\n",
      "\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\n",
      "Nietzsche\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in df_dominant_topic[df_dominant_topic['Dominant_Topic']==16]['Document_No']:\n",
    "    print(newsgroups_train.data[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinit\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type complex is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    339\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m             \u001b[1;31m# Finally look for special method names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyLDAvis\\_display.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(data, kwds)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformatters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text/html'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     formatter.for_type(PreparedData,\n\u001b[1;32m--> 313\u001b[1;33m                        lambda data, kwds=kwargs: prepared_data_to_html(data, **kwds))\n\u001b[0m\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyLDAvis\\_display.py\u001b[0m in \u001b[0;36mprepared_data_to_html\u001b[1;34m(data, d3_url, ldavis_url, ldavis_css_url, template_type, visid, use_http)\u001b[0m\n\u001b[0;32m    176\u001b[0m                            \u001b[0md3_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md3_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                            \u001b[0mldavis_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mldavis_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m                            \u001b[0mvis_json\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m                            ldavis_css_url=ldavis_css_url)\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36mto_json\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m        \u001b[1;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNumPyEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m         **kw).encode(obj)\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyLDAvis\\utils.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJSONEncoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m--> 179\u001b[1;33m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[0;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type complex is not JSON serializable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreparedData(topic_coordinates=                                x                             y  topics  \\\n",
       "topic                                                                     \n",
       "16       (0.07033726979353762+0j)    (-0.003890530691758721+0j)       1   \n",
       "2       (-0.03583687315833381+0j)      (0.05562143147016337+0j)       2   \n",
       "7       (-0.04444768368160779+0j)     (-0.05101206451055137+0j)       3   \n",
       "19     (0.0005851345321413966+0j)    (-4.22844863443095e-05+0j)       4   \n",
       "8      (0.0005851345321413966+0j)    (-4.22844863443095e-05+0j)       5   \n",
       "1      (0.0005851345321413966+0j)    (-4.22844863443095e-05+0j)       6   \n",
       "3      (0.0005851345321413966+0j)    (-4.22844863443095e-05+0j)       7   \n",
       "4      (0.0005851345321413966+0j)    (-4.22844863443095e-05+0j)       8   \n",
       "5      (0.0005851345321413934+0j)   (-4.228448634430845e-05+0j)       9   \n",
       "6       (0.000585134532141396+0j)   (-4.228448634430976e-05+0j)      10   \n",
       "9      (0.0005851345321413954+0j)  (-4.2284486344310294e-05+0j)      11   \n",
       "18     (0.0005851345321413954+0j)   (-4.228448634431134e-05+0j)      12   \n",
       "10     (0.0005851345321413979+0j)    (-4.22844863443095e-05+0j)      13   \n",
       "11     (0.0005851345321413979+0j)   (-4.228448634431002e-05+0j)      14   \n",
       "12     (0.0005851345321413947+0j)  (-4.2284486344310545e-05+0j)      15   \n",
       "13     (0.0005851345321414003+0j)   (-4.228448634431134e-05+0j)      16   \n",
       "14     (0.0005851345321413972+0j)  (-4.2284486344306614e-05+0j)      17   \n",
       "15     (0.0005851345321414003+0j)   (-4.228448634430687e-05+0j)      18   \n",
       "17     (0.0005851345321413903+0j)   (-4.228448634430923e-05+0j)      19   \n",
       "0      (0.0005851345321413903+0j)   (-4.228448634430923e-05+0j)      20   \n",
       "\n",
       "       cluster       Freq  \n",
       "topic                      \n",
       "16           1  55.104584  \n",
       "2            1  22.962770  \n",
       "7            1  20.737148  \n",
       "19           1   0.070323  \n",
       "8            1   0.070323  \n",
       "1            1   0.070323  \n",
       "3            1   0.070323  \n",
       "4            1   0.070323  \n",
       "5            1   0.070323  \n",
       "6            1   0.070323  \n",
       "9            1   0.070323  \n",
       "18           1   0.070323  \n",
       "10           1   0.070323  \n",
       "11           1   0.070323  \n",
       "12           1   0.070323  \n",
       "13           1   0.070323  \n",
       "14           1   0.070323  \n",
       "15           1   0.070323  \n",
       "17           1   0.070323  \n",
       "0            1   0.070323  , topic_info=     Category     Freq           Term     Total  loglift  logprob\n",
       "term                                                             \n",
       "7     Default  3.00000           door  3.000000  30.0000  30.0000\n",
       "17    Default  3.00000         lerxst  3.000000  29.0000  29.0000\n",
       "154   Default  2.00000         purdue  2.000000  28.0000  28.0000\n",
       "52    Default  2.00000          clock  2.000000  27.0000  27.0000\n",
       "96    Default  2.00000        anybody  2.000000  26.0000  26.0000\n",
       "149   Default  2.00000      powerbook  2.000000  25.0000  25.0000\n",
       "134   Default  2.00000        machine  2.000000  24.0000  24.0000\n",
       "107   Default  2.00000        display  2.000000  23.0000  23.0000\n",
       "123   Default  2.00000          heard  2.000000  22.0000  22.0000\n",
       "155   Default  2.00000       question  2.000000  21.0000  21.0000\n",
       "90    Default  2.00000     washington  2.000000  20.0000  20.0000\n",
       "194   Default  2.00000         harris  2.000000  19.0000  19.0000\n",
       "23    Default  1.00000          model  1.000000  18.0000  18.0000\n",
       "21    Default  1.00000           mail  1.000000  17.0000  17.0000\n",
       "33    Default  1.00000           spec  1.000000  16.0000  16.0000\n",
       "4     Default  1.00000         bumper  1.000000  15.0000  15.0000\n",
       "10    Default  1.00000      enlighten  1.000000  14.0000  14.0000\n",
       "31    Default  1.00000       separate  1.000000  13.0000  13.0000\n",
       "9     Default  1.00000         engine  1.000000  12.0000  12.0000\n",
       "11    Default  1.00000          funky  1.000000  11.0000  11.0000\n",
       "34    Default  1.00000          sport  1.000000  10.0000  10.0000\n",
       "22    Default  1.00000       maryland  1.000000   9.0000   9.0000\n",
       "3     Default  1.00000        brought  1.000000   8.0000   8.0000\n",
       "30    Default  1.00000           rest  1.000000   7.0000   7.0000\n",
       "0     Default  1.00000       addition  1.000000   6.0000   6.0000\n",
       "8     Default  1.00000          early  1.000000   5.0000   5.0000\n",
       "6     Default  1.00000        college  1.000000   4.0000   4.0000\n",
       "32    Default  1.00000          small  1.000000   3.0000   3.0000\n",
       "1     Default  1.00000           body  1.000000   2.0000   2.0000\n",
       "27    Default  1.00000           park  1.000000   1.0000   1.0000\n",
       "...       ...      ...            ...       ...      ...      ...\n",
       "188   Topic20  0.00099        command  1.161349   0.1926  -5.4072\n",
       "210   Topic20  0.00099           rjck  1.161349   0.1926  -5.4072\n",
       "220   Topic20  0.00099          world  1.161349   0.1926  -5.4072\n",
       "199   Topic20  0.00099       jonathan  1.161348   0.1926  -5.4072\n",
       "69    Topic20  0.00099        message  1.162288   0.1918  -5.4072\n",
       "80    Topic20  0.00099         shared  1.162287   0.1918  -5.4072\n",
       "79    Topic20  0.00099           send  1.162285   0.1918  -5.4072\n",
       "78    Topic20  0.00099      requested  1.162285   0.1918  -5.4072\n",
       "77    Topic20  0.00099         report  1.162289   0.1918  -5.4072\n",
       "76    Topic20  0.00099          rated  1.162287   0.1918  -5.4072\n",
       "75    Topic20  0.00099           qvfo  1.162287   0.1918  -5.4072\n",
       "74    Topic20  0.00099      procedure  1.162289   0.1918  -5.4072\n",
       "73    Topic20  0.00099           poll  1.973637  -0.3377  -5.4072\n",
       "72    Topic20  0.00099     oscillator  1.162288   0.1918  -5.4072\n",
       "68    Topic20  0.00099      knowledge  1.162289   0.1918  -5.4072\n",
       "82    Topic20  0.00099           sink  1.162290   0.1918  -5.4072\n",
       "67    Topic20  0.00099       keywords  1.162287   0.1918  -5.4072\n",
       "66    Topic20  0.00099           innc  1.162288   0.1918  -5.4072\n",
       "65    Topic20  0.00099           hour  1.162285   0.1918  -5.4072\n",
       "64    Topic20  0.00099           heat  1.162286   0.1918  -5.4072\n",
       "62    Topic20  0.00099         guykuo  1.568336  -0.1078  -5.4072\n",
       "61    Topic20  0.00099  functionality  1.162289   0.1918  -5.4072\n",
       "60    Topic20  0.00099         floppy  1.568335  -0.1078  -5.4072\n",
       "59    Topic20  0.00099          final  1.358591   0.0358  -5.4072\n",
       "58    Topic20  0.00099           fair  1.162288   0.1918  -5.4072\n",
       "57    Topic20  0.00099     experience  1.568340  -0.1078  -5.4072\n",
       "81    Topic20  0.00099        shelley  1.162287   0.1918  -5.4072\n",
       "83    Topic20  0.00099           soul  1.162288   0.1918  -5.4072\n",
       "55    Topic20  0.00099           disk  1.451133  -0.0301  -5.4072\n",
       "97    Topic20  0.00099        anymore  1.261417   0.1100  -5.4072\n",
       "\n",
       "[1295 rows x 6 columns], token_table=      Topic      Freq          Term\n",
       "term                               \n",
       "183       3  0.638659       abraxis\n",
       "42        2  0.860372  acceleration\n",
       "0         1  0.531181      addition\n",
       "184       3  0.861068       address\n",
       "185       3  0.638660         amber\n",
       "44        2  0.860371      answered\n",
       "96        1  0.875765       anybody\n",
       "97        1  0.792759       anymore\n",
       "46        2  0.860372      attained\n",
       "47        2  0.860372          base\n",
       "1         1  0.531181          body\n",
       "48        2  0.860372         brave\n",
       "49        2  0.860372         brief\n",
       "3         1  0.531181       brought\n",
       "4         1  0.531180        bumper\n",
       "5         1  0.531181        called\n",
       "50        2  0.860371          card\n",
       "51        2  0.637617        carson\n",
       "186       3  0.638661          chip\n",
       "187       3  0.861069         class\n",
       "52        2  0.718356         clock\n",
       "6         1  0.531181       college\n",
       "188       3  0.861068       command\n",
       "189       3  0.861069   corporation\n",
       "53        2  0.860372           day\n",
       "54        2  0.860372     detailing\n",
       "55        1  0.689117          disk\n",
       "107       1  0.875767       display\n",
       "108       1  0.870742  distribution\n",
       "190       3  0.638658      division\n",
       "...     ...       ...           ...\n",
       "210       3  0.861068          rjck\n",
       "211       3  0.861069        robert\n",
       "212       3  0.861069         scare\n",
       "79        2  0.860374          send\n",
       "213       3  0.861069         sense\n",
       "31        1  0.531180      separate\n",
       "80        2  0.860372        shared\n",
       "81        2  0.860373       shelley\n",
       "82        2  0.860370          sink\n",
       "32        1  0.531181         small\n",
       "83        2  0.860372          soul\n",
       "33        1  0.531180          spec\n",
       "84        2  0.637617         speed\n",
       "34        1  0.531181         sport\n",
       "214       3  0.861070         stuff\n",
       "85        2  0.860371   summarizing\n",
       "86        1  0.874125       summary\n",
       "215       3  0.638660        system\n",
       "37        1  0.905116        thanks\n",
       "39        1  0.905113    university\n",
       "87        2  0.637619       upgrade\n",
       "89        2  0.860371         usage\n",
       "216       3  0.861070          uucp\n",
       "217       3  0.861070       version\n",
       "90        2  0.840729    washington\n",
       "218       3  0.507675        weitek\n",
       "219       3  0.861070        winter\n",
       "220       3  0.861068         world\n",
       "221       3  0.861068        writes\n",
       "222       3  0.861070         wrote\n",
       "\n",
       "[126 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[17, 3, 8, 20, 9, 2, 4, 5, 6, 7, 10, 19, 11, 12, 13, 14, 15, 16, 18, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_tfidf, bow, dictionary=lda_model_tfidf.id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(doc):\n",
    "    bow_doc = dwords.doc2bow(lemmatize(preprocess(doc)))\n",
    "    for index, score in sorted(lda_model_tfidf[bow_test], key=lambda tup: -1*tup[1]):\n",
    "        print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))\n",
    "    \n",
    "    df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model_tfidf, corpus=bow, texts=preproc_doc)\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']    \n",
    "    return [i for i in df_dominant_topic['Keywords'] if i in df_dominant_topic['Text']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
